%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{PFFT Reference}\label{chap:ref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Files and Data Types}
You must include the PFFT header file by
\begin{lstlisting}
#include <pfft.h>
\end{lstlisting}
in the preamble of each source file that calls PFFT. This header automatically includes \code{fftw.h} and \code{fftw3-mpi.h}.
Therefore, PFFT can use the \code{fftw_complex} data type defined in \code{fftw.h}, see~\cite{fftw-cplx-num}.
Note that \code{fftw_complex} is defined to be the C99 native complex whenever \code{<complex.h>} is included \emph{before}
\code{<fftw.h>}, \code{<fftw-mpi.h>} and \code{<pfft.h>}. Otherwise it is defined as 
\begin{lstlisting}
typedef double fftw_complex[2];
\end{lstlisting}
For the sake of a clean namespace we define the wrapper data type \code{pfft_complex} as
\begin{lstlisting}
typedef fftw_complex pfft_complex;
\end{lstlisting}
that can be used equivallently to \code{fftw_complex}.
Futhermore, we define the wrapper functions
\begin{lstlisting}
void *pfft_malloc(size_t n);
double *pfft_alloc_real(size_t n);
pfft_complex *pfft_alloc_complex(size_t n);
void pfft_free(void *p);
\end{lstlisting}
as substitues for their corresponding FFTW equivalents, see~\cite{fftw-malloc}. 
Note that memory allocated by one
of these functions must be freed with \code{pfft_free} (or its equivalent \code{fftw_free}).
Because of the performance reasons given in~\cite{fftw-align-mem}
we recommend to use one of the \code{pfft_} (or its equivalent \code{fftw_}) allocation functions
for all arrays containing FFT inputs and outputs.
However, PFFT will also work (possibly slower) with any other memory allocation method.

Different precisions are handled as in FFTW: That is \code{pfft_} functions and datatypes become \code{pfftf_} (single precision)
or \code{pfftl_} (long double precision) prefixed. Quadruple precision is not yet supported. 
The main problem is that we do not know about a suitable MPI datatype to represent \code{__float128}.

\section{MPI Initialization}
Initialization and cleanup of PFFT in done in the same way as for FFTW-MPI, see~\cite{fftw-mpi-init}.
In order to keep a clean name space, PFFT offers the wrapper functions 
\begin{lstlisting}
void pfft_init(void);
void pfft_cleanup(void);
\end{lstlisting}
that can be used as substitutes for \code{fftw_mpi_init} and \code{fftw_mpi_cleanup}, respectively.

\section{Using PFFT Plans}
PFFT follows exactly the same workflow as FFTW-MPI. 
A plan created by one of the functions given in Section~\ref{sec:planning} is executed with
\begin{lstlisting}
void pfft_execute(const pfft_plan plan);
\end{lstlisting}
and freed with
\begin{lstlisting}
void pfft_destroy_plan(const pfft_plan plan);
\end{lstlisting}
Note, that you can \emph{not} apply \code{fftw_mpi_execute} or \code{fftw_destroy} on PFFT plans. 

The new array execute functions are given by
\begin{lstlisting}
void pfft_execute_dft(const pfft_plan plan, pfft_complex *in, pfft_complex *out);
void pfft_execute_dft_r2c(const pfft_plan plan, double *in, pfft_complex *out);
void pfft_execute_dft_c2r(const pfft_plan plan, pfft_complex *in, double *out);
void pfft_execute_r2r(const pfft_plan plan, double *in, double *out);
\end{lstlisting}
The arrays given by \code{in} and \code{out} must have the correct size
and the same alignement as the array that were used to create the plan, just as it is the case for FFTW, see~\ref{fftw-new-array}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Distribution Functions}\label{sec:local-size}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Complex-to-Complex FFT}
\begin{lstlisting}
ptrdiff_t pfft_local_size_dft_3d(
    const ptrdiff_t *n, MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t pfft_local_size_dft(
    int rnk_n, const ptrdiff_t *n,
    MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t pfft_local_size_many_dft(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
\end{lstlisting}

\begin{lstlisting}
void pfft_local_block_dft_3d(
    const ptrdiff_t *n, MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
void pfft_local_block_dft(
    int rnk_n, const ptrdiff_t *n,
    MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
void pfft_local_block_many_dft(
    int rnk_n, const ptrdiff_t *ni, const ptrdiff_t *no,
    const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
\end{lstlisting}

\subsection{Real-to-Complex FFT}
\begin{lstlisting}
ptrdiff_t pfft_local_size_dft_r2c_3d(
    const ptrdiff_t *n, MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t pfft_local_size_dft_r2c(
    int rnk_n, const ptrdiff_t *n,
    MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t pfft_local_size_many_dft_r2c(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
\end{lstlisting}

\begin{lstlisting}
void pfft_local_block_dft_r2c_3d(
    const ptrdiff_t *n, MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
void pfft_local_block_dft_r2c(
    int rnk_n, const ptrdiff_t *n,
    MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
void pfft_local_block_many_dft_r2c(
    int rnk_n, const ptrdiff_t *ni, const ptrdiff_t *no,
    const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
\end{lstlisting}

\subsection{Complex-to-Real FFT}
\begin{lstlisting}
ptrdiff_t pfft_local_size_dft_c2r_3d(
    const ptrdiff_t *n, MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t pfft_local_size_dft_c2r(
    int rnk_n, const ptrdiff_t *n,
    MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t pfft_local_size_many_dft_c2r(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
\end{lstlisting}

\begin{lstlisting}
void pfft_local_block_dft_c2r_3d(
    const ptrdiff_t *n, MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
void pfft_local_block_dft_c2r(
    int rnk_n, const ptrdiff_t *n,
    MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
void pfft_local_block_many_dft_c2r(
    int rnk_n, const ptrdiff_t *ni, const ptrdiff_t *no,
    const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
\end{lstlisting}

\subsection{Real-to-Real FFT}
\begin{lstlisting}
ptrdiff_t pfft_local_size_r2r_3d(
    const ptrdiff_t *n, MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t pfft_local_size_r2r(
    int rnk_n, const ptrdiff_t *n,
    MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t pfft_local_size_many_r2r(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    MPI_Comm comm_cart, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
\end{lstlisting}

\begin{lstlisting}
void pfft_local_block_r2r_3d(
    const ptrdiff_t *n, MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
void pfft_local_block_r2r(
    int rnk_n, const ptrdiff_t *n,
    MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
void pfft_local_block_many_r2r(
    int rnk_n, const ptrdiff_t *ni, const ptrdiff_t *no,
    const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    MPI_Comm comm_cart, int pid, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Plan Creation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Complex-to-Complex FFT}
\begin{lstlisting}
pfft_plan pfft_plan_dft_3d(
    const ptrdiff_t *n, pfft_complex *in, pfft_complex *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
pfft_plan pfft_plan_dft(
    int rnk_n, const ptrdiff_t *n, pfft_complex *in, pfft_complex *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
pfft_plan pfft_plan_many_dft(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    pfft_complex *in, pfft_complex *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
pfft_plan pfft_plan_many_dft_skipped(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    const int *skip_trafos, pfft_complex *in, pfft_complex *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
\end{lstlisting}

\subsection{Real-to-Complex FFT}
\begin{lstlisting}
pfft_plan pfft_plan_dft_r2c_3d(
    const ptrdiff_t *n, double *in, pfft_complex *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
pfft_plan pfft_plan_dft_r2c(
    int rnk_n, const ptrdiff_t *n, double *in, pfft_complex *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
pfft_plan pfft_plan_many_dft_r2c(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    double *in, pfft_complex *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
pfft_plan pfft_plan_many_dft_r2c_skipped(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    const int *skip_trafos, double *in, pfft_complex *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
\end{lstlisting}

\subsection{Complex-to-Real FFT}
\begin{lstlisting}
pfft_plan pfft_plan_dft_c2r_3d(
    const ptrdiff_t *n, pfft_complex *in, double *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
pfft_plan pfft_plan_dft_c2r(
    int rnk_n, const ptrdiff_t *n, pfft_complex *in, double *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
pfft_plan pfft_plan_many_dft_c2r(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    pfft_complex *in, double *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
pfft_plan pfft_plan_many_dft_c2r_skipped(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    const int *skip_trafos, pfft_complex *in, double *out, MPI_Comm comm_cart,
    int sign, unsigned pfft_flags);
\end{lstlisting}

\subsection{Real-to-Real FFT}
\begin{lstlisting}
pfft_plan pfft_plan_r2r_3d(
    const ptrdiff_t *n, double *in, double *out, MPI_Comm comm_cart,
    const pfft_r2r_kind *kinds, unsigned pfft_flags);
pfft_plan pfft_plan_r2r(
    int rnk_n, const ptrdiff_t *n, double *in, double *out, MPI_Comm comm_cart,
    const pfft_r2r_kind *kinds, unsigned pfft_flags);
pfft_plan pfft_plan_many_r2r(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    double *in, double *out, MPI_Comm comm_cart,
    const pfft_r2r_kind *kinds, unsigned pfft_flags);
pfft_plan pfft_plan_many_r2r_skipped(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
    ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
    const int *skip_trafos, double *in, double *out, MPI_Comm comm_cart,
    const pfft_r2r_kind *kinds, unsigned pfft_flags);
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{FFT Execution Timer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
PFFT offers an easy way to perform run time measurements and print/write the results.

\subsection{Basis Run Time Measurements}
PFFT-plans automatically accumulate the local run times of every call to \code{pfft_execute}.
For most applications it is sufficient to print run time of a plan \code{ths} averaged over all runs with
\begin{lstlisting}
void pfft_print_average_timer(
    const pfft_plan ths, MPI_Comm comm);
\end{lstlisting}
Note, that for each timer the maximum time over all processes is reduced to rank \code{0} of communicator \code{comm},
i.e., a call to \code{MPI_Reduce} is performed and the output is only printed on this process.
The following function works in the same way but prints more verbose output
\begin{lstlisting}
void pfft_print_average_timer_adv(
    const pfft_plan ths, MPI_Comm comm);
\end{lstlisting}

To write the averaged run time of plan \code{ths} into a file called \code{name} use
\begin{lstlisting}
void pfft_write_average_timer(
    const pfft_plan ths, const char *name, MPI_Comm comm);
void pfft_write_average_timer_adv(
    const pfft_plan ths, const char *name, MPI_Comm comm);
\end{lstlisting}
Again, the output is only written on rank \code{0} of communicator \code{comm}.

Discard all the recorded run times with
\begin{lstlisting}
void pfft_reset_timer(
    pfft_plan ths);
\end{lstlisting}
This function is called per default at the end of every PFFT plan creation function.

\subsection{Advanced Timer Manipulation}
In order to access the run times directly a new typedef \code{pfft_timer} is introduced.
The following function returns a copy of the timer corresponding to PFFT plan \code{ths}
\begin{lstlisting}
pfft_timer pfft_get_timer(
    const pfft_plan ths);
\end{lstlisting}
Note that the memory of the returned \code{pfft_timer} must be released with
\begin{lstlisting}
void pfft_destroy_timer(
    pfft_timer ths);
\end{lstlisting}
as soon as the timer is not needed anymore. 

In the following we introduce some routines to perform basic operations on timers.
For all functions with a \code{pfft_timer} return value you must use \code{pfft_destroy_timer}
in order to release the allocated memory of the timer.
Create a copy of a PFFT-timer \code{orig} with
\begin{lstlisting}
pfft_timer pfft_copy_timer(
    const pfft_timer orig);
\end{lstlisting}
Compute the average, local time over all runs of a timer \code{ths} with
\begin{lstlisting}
void pfft_average_timer(
    pfft_timer ths);
\end{lstlisting}
Create a new timer that contains the sum of two timers \code{sum1} and \code{sum2} with
\begin{lstlisting}
pfft_timer pfft_add_timers(
    const pfft_timer sum1, const pfft_timer sum2);
\end{lstlisting}
Create a timer that contains the maximum times of all the timers \code{ths} from all processes belonging to communicator \code{comm} with
\begin{lstlisting}
pfft_timer pfft_reduce_max_timer(
    const pfft_timer ths, MPI_Comm comm);
\end{lstlisting}
Since this function calls \code{MPI_Reduce}, only the first process (rank 0) of \code{comm} will get the desired data while all
the other processes have timers with undefined values.

Note, that you can not access the elements of a timer directly, since it is only a pointer to a \code{struct}.
However, PFFT offers a routine that creates an array and copies all the entries of the timer into it
\begin{lstlisting}
double* pfft_convert_timer2vec(
    const pfft_timer ths);
\end{lstlisting}
Remember to use \code{free} in order to release the allocated memory of the returned array at the moment it is not needed anymore.
The entries of the returned array are ordered as follows:
\begin{compactitem}
  \item dimension of the process mesh \code{rnk_pm}
  \item number of serial trafos \code{rnk_trafo}
  \item number of global remaps \code{rnk_remap}
  \item number of \code{pfft_execute} runs \code{iter}
  \item local run time of all runs
  \item \code{rnk_n} local times of the serial trafos
  \item \code{rnk_remap} local times of the global remaps
  \item 2 times of the global remaps that are only necessary for three-dimensional FFTs on three-dimensional process meshes
  \item time for computing twiddled input (as needed for \code{PFFT_SHIFTED_OUT}) 
  \item time for computing twiddled output (as needed for \code{PFFT_SHIFTED_IN}) 
\end{compactitem}
The complementary function
\begin{lstlisting}
pfft_timer pfft_convert_vec2timer(
    const double *times);
\end{lstlisting}
creates a timer and fills it's entries with the data from array \code{times}. Thereby, the entries of \code{times}
must be in the same order as above. 

\todo[inline]{implement getters and setters for pfft timer}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ghost Cell Communication}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the following we describe the PFFT ghost cell communication module.
At the moment, PFFT ghost cell communication is restricted to three-dimensional arrays.
Let us know if would like to have ghost cell communication also for higher dimensions.

Assume a three-dimensional array \code{data} of size \code{n[3]} that is distributed in blocks such that each process has a local copy of
\code{data[k[0],k[1],k[2]]} with 
\begin{lstlisting}
local_start[t] <= k[t] < local_start[t] + local_n[t] /* t=0,1,2 */
\end{lstlisting}
The ``classical'' ghost cell exchange communicates all the necessary data between neighboring processes,
such that each process gets a local copy of \code{data[k[0],k[1],k[2]]} with
\begin{lstlisting}
local_gc_start[t] <= k[t] < local_gc_start[t] + local_ngc[t] /* t=0,1,2 */
\end{lstlisting}
where
\begin{lstlisting}
local_gc_start[t] = local_start[t] - gc_below[t]; /* t=0,1,2 */
local_ngc[t] = local_n[t] + gc_below[t] + gc_above[t]; /* t=0,1,2 */
\end{lstlisting}
I.e., the local array block is increased in every dimension by \code{gc_below} elements below and \code{gc_above} elements above.
Hereby, the \code{data} is wrapped periodically whenever \code{k[t]} exceeds the array dimensions.
The number of ghost cells in every dimension can be chosen independently and can be arbitrary large, i.e.,
PFFT ghost cell communication also handles the case where the requested data exceeds next neighbor communication.
The number of ghost cells can even be bigger than the array size, which results in multiple local copies of the same data elements at every process. 

\subsection{Using Ghost Cell Plans}
We introduce a new datatype \code{pfft_gcplan} that stores all the necessary information for ghost cell communication.
Using a ghost cell plan follows the typical workflow: At first, determine the parallel data distribution; cf. Section~\ref{sec:gc:local-size}.
Next, create a ghost cell plan; cf. Section~\ref{sec:gc:plan-cdata} and Section~\ref{sec:gc:plan-rdata}. 
Execute the ghost cell communication with one of the following two functions
\begin{lstlisting}
void pfft_exchange(
    pfft_gcplan ths);
void pfft_reduce(
    pfft_gcplan ths);
\end{lstlisting}
Hereby, a ghost cell exchange creates duplicates of local data elements on next neighboring processes,  
while a ghost cell reduce is the adjoint counter part of the exchange, i.e.,
it adds the sum of all the duplicates of a local data element to the original data element.

If the plan is not needed anymore, free the allocated memory with
\begin{lstlisting}
void pfft_destroy_gcplan(
    pfft_gcplan ths);
\end{lstlisting}
It is invalid to pass a freed plan to \code{pfft_exchange} or \code{pfft_reduce}.

\subsection{Data Distribution}\label{sec:gc:local-size}
For a given three-dimensional local array of size \code{local_n} with offset \code{local_n_start} the following function
\begin{lstlisting}
ptrdiff_t pfft_local_size_gc_3d(
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    ptrdiff_t alloc_local, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    ptrdiff_t *local_ngc, ptrdiff_t *local_gc_start);
\end{lstlisting}
computes the local array plus ghost cell size \code{local_ngc} and the corresponding offset \code{local_gc_start}.
The three-dimensional arrays \code{gc_below} and \code{gc_above} give the number of ghost cells below and above the local array.
Return value is the necessary memory to store the array plus ghost cells.
Hereby, \code{alloc_local}, \code{local_n}, \code{local_start} should be the return values of an appropriate
\code{pfft_local_size} invocation; cf. Section~\ref{sec:local-size}.
A generalization of the three-dimensional interface to \code{rnk_n}-dimensionsinal arrays is given by
\begin{lstlisting}
ptrdiff_t pfft_local_size_gc(
    int rnk_n, 
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    ptrdiff_t alloc_local,
    const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    ptrdiff_t *local_ngc, ptrdiff_t *local_gc_start);
\end{lstlisting}
Hereby, all \code{ptrdiff_t} arrays are of length \code{rnk_n}.
However, only the case \code{rnk_n==3} is completely implemented at the moment. 

You can substitute each data element in a ghost cell communication by tuples of size \code{howmany} with
\begin{lstlisting}
ptrdiff_t pfft_local_size_many_gc(
    int rnk_n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    ptrdiff_t alloc_local, ptrdiff_t howmany,
    const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    ptrdiff_t *local_ngc, ptrdiff_t *local_gc_start);
\end{lstlisting}


Works for complex and real arrays. R2c FFTs use the complex array decomposition for padded real values and the real array decomposition for non-padded real values.



\subsection{Plan Creation for Complex Data}\label{sec:gc:plan-cdata}
Data distribution functions are the same for complex and real data, see Section~\ref{sec:gc:local_size}.

\begin{lstlisting}
pfft_gcplan pfft_plan_cgc_3d(
    const ptrdiff_t *n,
    const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    pfft_complex *data, MPI_Comm comm_cart, unsigned gc_flags);
pfft_gcplan pfft_plan_cgc(
    int rnk_n, const ptrdiff_t *n,
    const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    pfft_complex *data, MPI_Comm comm_cart, unsigned gc_flags);
pfft_gcplan pfft_plan_many_cgc(
    int rnk_n, const ptrdiff_t *n,
    ptrdiff_t howmany, const ptrdiff_t *block,
    const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    pfft_complex *data, MPI_Comm comm_cart, unsigned gc_flags);
\end{lstlisting}


\subsection{Plan Creation for Real Data}\label{sec:gc:plan-rdata}
Data distribution functions are the same for complex and real data, see Section~\ref{sec:gc:local_size}.

\begin{lstlisting}
pfft_gcplan pfft_plan_rgc_3d(
    const ptrdiff_t *n,
    const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    double *data, MPI_Comm comm_cart, unsigned gc_flags);
pfft_gcplan pfft_plan_rgc(
  int rnk_n, const ptrdiff_t *n,
  const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
  double *data, MPI_Comm comm_cart, unsigned gc_flags);
pfft_gcplan pfft_plan_many_rgc(
  int rnk_n, const ptrdiff_t *n,
  ptrdiff_t howmany, const ptrdiff_t *block,
  const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
  double *data, MPI_Comm comm_cart, unsigned gc_flags);
\end{lstlisting}

\subsection{Ghost Cell Execution Timer}
PFFT ghost cell plans automatically accumulate the local run times of every call to \code{pfft_exchange} and \code{pfft_reduce}.
For most applications it is sufficient to print run time of a plan \code{ths} averaged over all runs with
\begin{lstlisting}
void pfft_print_average_gctimer(
    const pfft_gcplan ths, MPI_Comm comm);
\end{lstlisting}
Note, that for each timer the maximum time over all processes is reduced to rank \code{0} of communicator \code{comm},
i.e., a call to \code{MPI_Reduce} is performed and the output is only printed on this process.
The following function works in the same way but prints more verbose output
\begin{lstlisting}
void pfft_print_average_gctimer_adv(
    const pfft_gcplan ths, MPI_Comm comm);
\end{lstlisting}

To write the averaged run time of a ghost cell plan \code{ths} into a file called \code{name} use
\begin{lstlisting}
void pfft_write_average_gctimer(
    const pfft_gcplan ths, const char *name, MPI_Comm comm);
void pfft_write_average_gctimer_adv(
    const pfft_gcplan ths, const char *name, MPI_Comm comm);
\end{lstlisting}
Again, the output is only written on rank \code{0} of communicator \code{comm}.

Discard all the recorded run times with
\begin{lstlisting}
void pfft_reset_gctimers(
    pfft_gcplan ths);
\end{lstlisting}
This function is called per default at the end of every ghost cell plan creation function.

In order to access the run times directly a new typedef \code{pfft_timer} is introduced.
The following functions return a copy of the timer corresponding to ghost cell plan \code{ths} that accumulated
the time for ghost cell exchange or ghost cell reduce, respectively:
\begin{lstlisting}
pfft_gctimer pfft_get_gctimer_exg(
    const pfft_gcplan ths);
pfft_gctimer pfft_get_gctimer_red(
    const pfft_gcplan ths);
\end{lstlisting}
Note that the memory of the returned \code{pfft_gctimer} must be released with
\begin{lstlisting}
void pfft_destroy_gctimer(
    pfft_gctimer ths);
\end{lstlisting}
as soon as the timer is not needed anymore. 

In the following we introduce some routines to perform basic operations on timers.
For all functions with a \code{pfft_gctimer} return value you must use \code{pfft_destroy_gctimer}
in order to release the allocated memory of the timer.
Create a copy of a ghost cell timer \code{orig} with
\begin{lstlisting}
pfft_gctimer pfft_copy_gctimer(
    const pfft_gctimer orig);
\end{lstlisting}
Compute the average, local time over all runs of a timer \code{ths} with
\begin{lstlisting}
void pfft_average_gctimer(
    pfft_gctimer ths);
\end{lstlisting}
Create a new timer that contains the sum of two timers \code{sum1} and \code{sum2} with
\begin{lstlisting}
pfft_gctimer pfft_add_gctimers(
    const pfft_gctimer sum1, const pfft_gctimer sum2);
\end{lstlisting}
Create a timer that contains the maximum times of all the timers \code{ths} from all processes belonging to communicator \code{comm} with
\begin{lstlisting}
pfft_gctimer pfft_reduce_max_gctimer(
    const pfft_gctimer ths, MPI_Comm comm);
\end{lstlisting}
Since this function calls \code{MPI_Reduce}, only the first process (rank 0) of \code{comm} will get the desired data while all
the other processes have timers with undefined values.

Note, that you can not access the elements of a timer directly, since it is only a pointer to a \code{struct}.
However, PFFT offers a routine that creates an array and copies all the entries of the timer into it
\begin{lstlisting}
void pfft_convert_gctimer2vec(
    const pfft_gctimer ths, double *times);
\end{lstlisting}
Remember to use \code{free} in order to release the allocated memory of the returned array at the moment it is not needed anymore.
The entries of the returned array are ordered as follows:
\begin{compactitem}
  \item number of \code{pfft_execute} runs \code{iter}
  \item local run time of all runs
  \item local run time of zero padding (make room for incoming ghost cells and init with zeros)
  \item local run time of the ghost cell exchange or reduce (depending on the timer)
\end{compactitem}
The complementary function
\begin{lstlisting}
pfft_gctimer pfft_convert_vec2gctimer(
    const double *times);
\end{lstlisting}
creates a timer and fills it's entries with the data from array \code{times}. Thereby, the entries of \code{times}
must be in the same order as above. 

\todo[inline]{implement getters and setters for ghost cell timer}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Useful Tools}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following functions are useful tools but are not necessarily needed to perform parallel FFTs.

\subsection{Initializing Complex Inputs and Checking Outputs}\label{sec:init-data-3d-c2c}
To fill a complex array \code{data} with reproducible, complex values you can use one of the functions
\begin{lstlisting}
void pfft_init_input_complex_3d(
    const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    pfft_complex *data);
void pfft_init_input_complex(
    int rnk_n, const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    pfft_complex *data);
\end{lstlisting}
Hereby, the arrays \code{n}, \code{local_n} and \code{local_n_start} of length \code{rnk_n} (\code{rnk_n==3} for \code{_3d})
give the size of the FFT, the local array size and the local array offset
as computed by the array distribution functions described in Section~\ref{sec:local-size}
The functions
\begin{lstlisting}
double pfft_check_output_complex_3d(
    const ptrdiff_t *n, 
    const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    const pfft_complex *data, MPI_Comm comm);
double pfft_check_output_complex(
    int rnk_n, const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    const pfft_complex *data, MPI_Comm comm);
\end{lstlisting}
compute the $l_1$-norm between the elements of array \code{data} and values produced by \code{pfft_init_input_complex_3d}, \code{pfft_init_input_complex}.

Note, that these functions can be combined for a quick consistency check of the FFT.
Since a forward FFT followed by a backward FFT reproduces the inputs up to a scaling factor,
the following code snippet should give a result equal to zero up to machine precision.
\begin{lstlisting}
/* Initialize input with random numbers */
pfft_init_input_complex_3d(n, local_ni, local_i_start,
    in);

/* execute parallel forward FFT */
pfft_execute(plan_forw);

/* execute parallel backward FFT */
pfft_execute(plan_back);

/* Scale data */
for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)
  in[l] /= (n[0]*n[1]*n[2]);

/* Print error of back transformed data */
err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);
pfft_printf(comm_cart_2d, "Error after one forward and backward trafo of size n=(%td, %td, %td):\n", n[0], n[1], n[2]);
pfft_printf(comm_cart_2d, "maxerror = %6.2e;\n", err);
\end{lstlisting}




\subsection{Initializing Real Inputs and Checking Outputs}\label{sec:init-data-3d-r2r}
To fill a real array \code{data} with reproducible, real values use one of the functions
\begin{lstlisting}
void pfft_init_input_real_3d(
    const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    double *data);
void pfft_init_input_real(
    int rnk_n, const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    double *data);
\end{lstlisting}
Hereby, the arrays \code{n}, \code{local_n} and \code{local_n_start} give the size of the FFT, the local array size and the local array offset
as computed by the array distribution functions described in Section~\ref{sec:local-size}
The functions
\begin{lstlisting}
double pfft_check_output_real_3d(
    const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    const pfft_complex *data, MPI_Comm comm);
double pfft_check_output_real(
    int rnk_n, const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    const pfft_complex *data, MPI_Comm comm);
\end{lstlisting}
compute the $l_1$-norm between the elements of array \code{data} and values produced by \code{pfft_init_input_real_3d}, \code{pfft_init_input_real}.

Note, that both \code{pfft_init_input_real*} functions will set all array elements to zero were \code{local_n + local_n_start >= n}.
In addition, both \code{pfft_check_output_real*} function will ignore all the errors resulting from these elements.
Therefore, it is safe to use all these functions for a consistency check of a r2c transform followed by a c2r transform since all padding elements will be ignored.


\subsection{Initializing r2c/c2r Inputs and Checking Outputs}\label{sec:init-data-3d-r2c}
The real inputs of a r2c transform can be initialized with the functions decribed in Section~\ref{sec:init-data-3d-r2r}.
However, generating suitable inputs for a c2r transform requires more caution.
In order to get real valued results of a DFT the complex input coefficients need to satisfy an radial Hermitian symmetry, i.e., $X[\mathbf k] = {X^*[-\mathbf k]}$.
We use the following trick to generate the complex input values for c2r transforms.
Assume any $\mathbf N$-periodic complex valued function $f$. It can be easily shown that the values
$X[\mathbf k] := \frac{1}{2}\left(f(\mathbf k)+f^*(-\mathbf k)\right)$ satisfy the radial Hermitian symmetry. 

To fill a complex array \code{data} with reproducible, complex values that fulfill the radial Hermitian symmetry use one of the functions
\begin{lstlisting}
void pfft_init_input_complex_hermitian_3d(
    const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    double *data);
void pfft_init_input_complex_hermitian(
    int rnk_n, const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    double *data);
\end{lstlisting}
Hereby, the arrays \code{n}, \code{local_n} and \code{local_n_start} give the size of the FFT, the local array size and the local array offset
as computed by the array distribution functions described in Section~\ref{sec:local-size}
The functions
\begin{lstlisting}
double pfft_check_output_complex_hermitian_3d(
    const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    const pfft_complex *data, MPI_Comm comm);
double pfft_check_output_complex_hermitian(
    int rnk_n, const ptrdiff_t *n,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    const pfft_complex *data, MPI_Comm comm);
\end{lstlisting}
compute the $l_1$-norm between the elements of array \code{data} and values produced by \code{pfft_init_input_complex_hermitian_3d}, \code{pfft_init_input_complex_hermitian}.

Note, that these functions can also be used in order to generate complex inputs with radial Hermitian symmetry for ordinary c2c transforms. 
Of course the results of such a c2c DFT will have all imaginary parts equal to zero up to machine precision.




\subsection{Operations on Arrays of Type \code{ptrdiff_t}}
The following routines are shortcuts for the elementwise manipulation of \code{ptrdiff_t} valued arrays.
In the following, all arrays \code{vec}, \code{vec1}, and \code{vec2} are of length \code{d} and type \code{ptrdiff_t}.
\begin{lstlisting}
ptrdiff_t pfft_prod_INT(
    int d, const ptrdiff_t *vec);
\end{lstlisting}
Returns the product over all elements of \code{vec}.
\begin{lstlisting}
ptrdiff_t pfft_sum_INT(
    int d, const ptrdiff_t *vec);
\end{lstlisting}
Returns the sum over all elements of \code{vec}.
\begin{lstlisting}
int pfft_equal_INT(
    int d, const ptrdiff_t *vec1, const ptrdiff_t *vec2);
\end{lstlisting}
Returns 1 if both arrays have equal entries, 0 otherwise.
\begin{lstlisting}
void pfft_vcopy_INT(
    int d, const ptrdiff_t *vec1,
    ptrdiff_t *vec2);
\end{lstlisting}
Copies the elements of \code{vec1} into \code{vec2}.
\begin{lstlisting}
void pfft_vadd_INT(
    int d, const ptrdiff_t *vec1, const ptrdiff_t *vec2,
    ptrdiff_t *sum);
\end{lstlisting}
Fills \code{sum} with the componentwise sum of \code{vec1} and \code{vec2}.
\begin{lstlisting}
void pfft_vsub_INT(
    int d, const ptrdiff_t *vec1, const ptrdiff_t *vec2,
    ptrdiff_t *sum);
\end{lstlisting}
Fills \code{sum} with the componentwise difference of \code{vec1} and \code{vec2}.

\subsection{Print Three-Dimensional Arrays in Parallel}
Use the following routine to print the elements of a block decomposed three-dimensional (real or complex valued) array \code{data} in a nicely formatted way.
\begin{lstlisting}
void pfft_apr_real_3d(
    const double *data,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    const char *name, MPI_Comm comm);
void pfft_apr_complex_3d(
    const pfft_complex *data,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    const char *name, MPI_Comm comm);
\end{lstlisting}
Obviously, this makes only sense for arrays of moderate size.
The block decomposition is given by \code{local_n}, \code{local_n_start} as returned
by the array distribution function decribed in Section~\ref{sec:local-size}.
Furthermore, some arbitrary string \code{name} can be added at the beginning of each output - typically this will be the name of the array.
Communicator \code{comm} must be suitable to the block decomposition and is used to synchronize the outputs over all processes.

Generalizations for the case where the dimensions of the local arrays are permuted are given by
\begin{lstlisting}
void pfft_apr_real_permuted_3d(
    const double *data,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    int perm0, int perm1, int perm2,
    const char *name, MPI_Comm comm);
void pfft_apr_complex_permuted_3d(
    const pfft_complex *data,
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    int perm0, int perm1, int perm2,
    const char *name, MPI_Comm comm);
\end{lstlisting}
Hereby, \code{perm0}, \code{perm1}, and \code{perm2} give the array's permutation of dimension.





\subsection{Reading Command Line Arguments}
The following function offers a simple way to read command line arguments into an array \code{parameter}.
\begin{lstlisting}
void pfft_get_args(
    int argc, char **argv, const char *name,
    int neededArgs, unsigned type,
    void *parameter);
\end{lstlisting}
Hereby, \code{argc} and \code{argv} are the standard argument of the \code{main} routine.
Furthermore, \code{name}, \code{neededAgrs}, and \code{type} give the name, number of entries and the type of the command line argument.
Supported types are \code{PFFT_INT}, \code{PFFT_PTRDIFF_T}, \code{PFFT_FLOAT}, \code{PFFT_DOUBLE}, and \code{PFFT_UNSIGNED},
which denote the standard C type that is used for typecasting.
The array \code{parameter} must be of sufficient size to hold \code{neededArgs} elements of the given data type.

For example, a program \code{test.c} containing the following code snippet
\begin{lstlisting}
double x;
pfft_get_args(argc, argv, "-pfft_x", 1, PFFT_DOUBLE, &x);
int np[2];
pfft_get_args(argc, argv, "-pfft_np", 2, PFFT_INT, np);
ptrdiff_t n[3];
pfft_get_args(argc, argv, "-pfft_n", 3, PFFT_PTRDIFF_T, n);
\end{lstlisting}
and run with
\begin{lstlisting}
./test -pfft_x 3.1 -pfft_np 2 3 -pfft_n 8 16 32
\end{lstlisting}
will read \code{x=3.1}, \code{np[2] = \{2,3\}}, and \code{n[3]=\{8,16,32\}}.
The argument order of appearance in the command line is irrelevant.
Note the address operator \code{&} in front of \code{x} in the second line!

\subsection{Parallel Substitutes for \code{vprintf}, \code{fprintf}, and \code{printf}}\label{sec:fprintf}
The following functions are similar to the standard C function \code{vfprintf}, \code{fprintf} and \code{printf} with the exception,
that only rank \code{0} within the given communicator \code{comm} will produce output.
The intension is to avoid the flood of messages that is produced when simple \code{printf} statement are run in parallel.
\begin{lstlisting}
void pfft_vfprintf(
    MPI_Comm comm, FILE *stream, const char *format, va_list ap);
void pfft_fprintf(
    MPI_Comm comm, FILE *stream, const char *format, ...);
void pfft_printf(
    MPI_Comm comm, const char *format, ...);
\end{lstlisting}

\section{Generating Periodic Cartesian Communicators}\label{sec:create-comm}

Based on the processes that are part of the given communicator \code{comm} the following routine
\begin{lstlisting}
int pfft_create_procmesh_1d(
    MPI_Comm comm, int np0,
    MPI_Comm *comm_cart_1d);
\end{lstlisting}
allocates and creates a one-dimensional, periodic, Cartesian communicator \code{comm_cart_1d} of size \code{np0}.
Thereby, a non-zero error code is returned whenever \code{np0} does not fit the size of \code{comm}.
The memory of the generated communicator should be released with \code{MPI_Comm_free} after usage.
Analogously, use 
\begin{lstlisting}
int pfft_create_procmesh_2d(
    MPI_Comm comm, int np0, int np1,
    MPI_Comm *comm_cart_2d);
\end{lstlisting}
in order to allocate and create two-dimensional, periodic, Cartesian communicator \code{comm_cart_2d} of size \code{np0*np1} or
\begin{lstlisting}
int pfft_create_procmesh(
    int rnk_np, MPI_Comm comm, const int *np,
    MPI_Comm *comm_cart);
\end{lstlisting}
in order to allocate and create a \code{rnk_np}-dimensional, periodic, Cartesian communicator of size \code{np[0]*np[1]*...*np[rnk_np-1]}.
Hereby, \code{np} is an array of length \code{rnk_np}.
Again, the memory of the generated communicator should be released with \code{MPI_Comm_free} after usage.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ghost Cell Communicaton}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Determine the Parallel Ghost Cell Layout}

Similarly to the

In general, local ghost cell layout is characterized by the local array size $local_ngc[rnk_n]$ and
the local offset \code{local_gc_start[rnk_n]}. Every process gets the data

A process owns the local array data
\code{data[k[0],k[1],k[2]]} with \code{local_start[t] <= k[t] < local_start[t] + local_n[t]},
add the ghost cells below and above, such that
\code{data[k[0],k[1],k[2]]} with \code{gc_below + local_start[t] <= k[t] < local_start[t] + local_n[t]},

Hereby, \code{local_gc_start[t] = local_start[t] - gc_below[t]} and \code{local_ngc[t] = local_n[t] + local_gc_below[t] + local_gc_above[t]}.

\code{data[k[0],k[1],k[2]]} with \code{local_gc_start[t] <= k[t] < local_gc_start[t] + local_ngc[t]}

The output arrays \code{local_ngc} and \code{local_gc_start} give the size and the offset of the local array plus ghost cells, i.e.,
every process owns the data \code{data[k[0],k[1],k[2]]} with
\begin{lstlisting}
local_gc_start[t] <= k[t] < local_gc_start[t] + local_ngc[t]
\end{lstlisting}
for \code{t=0,...,rnk-1}.
Hereby, the index \code{k} wraps periodically modulo \code{n} such that \code{0 <= k[t] < n[t]}.

\begin{lstlisting}
ptrdiff_t pfft_local_size_gc_3d(
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    ptrdiff_t alloc_local, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    ptrdiff_t *local_ngc, ptrdiff_t *local_gc_start);
\end{lstlisting}
For a given local array of size \code{local_n} with offset \code{local_n_start} compute the
local array plus ghost cell size \code{local_ngc} and offset \code{local_gc_start}.
The number of ghost cells below and above the local array are given by
Return value is the necessary memory to store the array plus ghost cells.
Hereby, \code{alloc_local} should be the return value of an appropriate
call of a \code{pff_local_size} function, see \ref{sec:local-size}.

\begin{lstlisting}
ptrdiff_t pfft_local_size_gc(
    int rnk_n, const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    ptrdiff_t alloc_local, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    ptrdiff_t *local_ngc, ptrdiff_t *local_gc_start);
\end{lstlisting}
Generalize the three-dimensional interface to \code{rnk} dimensions.
\begin{lstlisting}
ptrdiff_t pfft_local_size_many_gc(
    int rnk_n, const ptrdiff_t *local_n, const ptrdiff_t *local_start, ptrdiff_t alloc_local,
    ptrdiff_t howmany, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    ptrdiff_t *local_ngc, ptrdiff_t *local_gc_start);
\end{lstlisting}

\begin{lstlisting}
pfft_gcplan pfft_plan_rgc_3d(
    const ptrdiff_t *n, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    double *data, MPI_Comm comm_cart, unsigned gc_flags);
pfft_gcplan pfft_plan_cgc_3d(
    const ptrdiff_t *n, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    pfft_complex *data, MPI_Comm comm_cart, unsigned gc_flags);
pfft_gcplan pfft_plan_rgc(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    double *data, MPI_Comm comm_cart, unsigned gc_flags);
pfft_gcplan pfft_plan_cgc(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    pfft_complex *data, MPI_Comm comm_cart, unsigned gc_flags);
pfft_gcplan pfft_plan_many_rgc(
    int rnk_n, const ptrdiff_t *n, ptrdiff_t howmany, const ptrdiff_t *block,
    const ptrdiff_t *gc_below, const ptrdiff_t *gc_above, double *data, MPI_Comm comm_cart,
    unsigned gc_flags);
pfft_gcplan pfft_plan_many_cgc(
    int rnk_n, const ptrdiff_t *n, ptrdiff_t howmany, const ptrdiff_t *block,
    const ptrdiff_t *gc_below, const ptrdiff_t *gc_above, pfft_complex *data, MPI_Comm comm_cart,
    unsigned gc_flags);
\end{lstlisting}

\begin{lstlisting}
void pfft_exchange(
    pfft_gcplan ths);
void pfft_reduce(
    pfft_gcplan ths);
void pfft_destroy_gcplan(
    pfft_gcplan ths);
\end{lstlisting}

\subsection{Ghost Cell Timers}
\begin{lstlisting}
void pfft_reset_gctimers(
    pfft_gcplan ths);
pfft_gctimer pfft_get_gctimer_exg(
    const pfft_gcplan ths);
pfft_gctimer pfft_get_gctimer_red(
    const pfft_gcplan ths);
void pfft_print_average_gctimer(
    const pfft_gcplan ths, MPI_Comm comm);
void pfft_print_average_gctimer_adv(
    const pfft_gcplan ths, MPI_Comm comm);
void pfft_write_average_gctimer(
    const pfft_gcplan ths, const char *name, MPI_Comm comm);
void pfft_write_average_gctimer_adv(
    const pfft_gcplan ths, const char *name, MPI_Comm comm);
\end{lstlisting}

\begin{lstlisting}
pfft_gctimer pfft_copy_gctimer(
    const pfft_gctimer orig);
void pfft_average_gctimer(
    pfft_gctimer ths);
pfft_gctimer pfft_add_gctimers(
    const pfft_gctimer sum1, const pfft_gctimer sum2);
pfft_gctimer pfft_reduce_max_gctimer(
    const pfft_gctimer ths, MPI_Comm comm);
void pfft_convert_gctimer2vec(
    const pfft_gctimer ths, double *times);
pfft_gctimer pfft_convert_vec2gctimer(
    const double *times);
void pfft_destroy_gctimer(
    pfft_gctimer ths);
\end{lstlisting}
